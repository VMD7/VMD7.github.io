---
title: ğ——ğ—²ğ—²ğ—½ ğ—¦ğ—²ğ—²ğ—¸ ğ˜„ğ—¶ğ˜ğ—µğ—¶ğ—» ğ——ğ—²ğ—²ğ—½ğ—¦ğ—²ğ—²ğ—¸-ğ—¥ğŸ­ğŸš€
tags: [LLM, DeepSeek]
style: fill
color: info
description: DeepSeek has introduced three major innovations in their latest research on the DeepSeek-R1 model, taking AI reasoning and performance to new heights. Here's a detailed breakdown of these exciting advancements
#external_url: https://blog.usejournal.com/how-to-undo-your-git-failure-b76e31ecac74
---

DeepSeek has introduced three major innovations in their latest research on the [DeepSeek-R1 model](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf), taking AI reasoning and performance to new heights. Here's a detailed breakdown of these exciting advancements:

## ğ——ğ—²ğ—²ğ—½ğ—¦ğ—²ğ—²ğ—¸-ğ—¥ğŸ­-ğ—­ğ—²ğ—¿ğ—¼
* ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—¼ğ—» ğ—Ÿğ—®ğ—¿ğ—´ğ—²-ğ—¦ğ—°ğ—®ğ—¹ğ—² ğ—¥ğ—²ğ—¶ğ—»ğ—³ğ—¼ğ—¿ğ—°ğ—²ğ—ºğ—²ğ—»ğ˜ ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ (ğ—¥ğ—Ÿ): Built on DeepSeek V3, this model leverages the GRPO (Group Relative Policy Optimization) RL framework for improved reasoning.
* ğ—šğ—¥ğ—£ğ—¢ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸: Focuses on relative performance within groups, setting new benchmarks in AI reasoning. First introduced in April 2024, GRPO improves performance through step-by-step refinement, much like a group of students learning together.
* ğ—¥ğ—Ÿ ğ—¢ğ˜ƒğ—²ğ—¿ ğ—¦ğ—™ğ—§: DeepSeek-R1-Zero relies entirely on RL, bypassing Supervised Fine-Tuning (SFT) to avoid complications like reward hacking and additional resource needs. The model doesnâ€™t use a reward model, simplifying the training pipeline.
* ğ—£ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—¦ğ˜ğ˜†ğ—¹ğ—²: Reasoning is enclosed in <ğ˜ğ—µğ—¶ğ—»ğ—¸></ğ˜ğ—µğ—¶ğ—»ğ—¸>, and the answer in <ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿></ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿>, ensuring clear separation of thought process and output during training.
* ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—šğ—®ğ—¶ğ—»ğ˜€: AIME improved from 15.6% to 71% after thousands of RL steps, with majority voting boosting accuracy to 86.7%. However, challenges remain in readability and language consistency.

## ğ——ğ—²ğ—²ğ—½ğ—¦ğ—²ğ—²ğ—¸-ğ—¥ğŸ­
* ğ—¥ğ—Ÿ + ğ—–ğ—¼ğ—¹ğ—±-ğ—¦ğ˜ğ—®ğ—¿ğ˜ ğ——ğ—®ğ˜ğ—®: Combines RL with a small amount of cold-start data in a multi-stage training process, with CoT (Chain-of-Thought) examples guiding reasoning.
* ğ—¥ğ—²ğ—®ğ—±ğ—®ğ—¯ğ—¹ğ—² ğ—¢ğ˜‚ğ˜ğ—½ğ˜‚ğ˜: Uses a response pattern with each answer summarizing the reasoning process, defined as: 
|ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—®ğ—¹_ğ˜ğ—¼ğ—¸ğ—²ğ—»|<ğ—¿ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´_ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€>|ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—®ğ—¹_ğ˜ğ—¼ğ—¸ğ—²ğ—»|<ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ˜†>.
* ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—”ğ—½ğ—½ğ—¿ğ—¼ğ—®ğ—°ğ—µ: Includes a few-shot prompting technique and post-processing by human annotators for clarity. The model is trained with 600k reasoning-related and 200k non-reasoning samples, finetuning DeepSeek-V3-Base over 2 epochs.
* ğ—¥ğ—²ğ˜„ğ—®ğ—¿ğ—±ğ˜€ ğ—³ğ—¼ğ—¿ ğ—–ğ—¼ğ—»ğ˜€ğ—¶ğ˜€ğ˜ğ—²ğ—»ğ—°ğ˜†: RL training is guided by rewards for helpfulness, harmlessness, and language consistency.

## ğ——ğ—¶ğ˜€ğ˜ğ—¶ğ—¹ğ—¹ğ—²ğ—± ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€
* ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—•ğ—¿ğ—²ğ—®ğ—¸ğ˜ğ—µğ—¿ğ—¼ğ˜‚ğ—´ğ—µğ˜€: Distilled models for both 32B and 70B architectures have set new performance benchmarks.
* ğ—¦ğ—™ğ—§-ğ—•ğ—®ğ˜€ğ—²ğ—±: These models are distilled using 800k samples and rely solely on Supervised Fine-Tuning (SFT), leaving the exploration of RL for the community.
* ğ—™ğ˜‚ğ˜ğ˜‚ğ—¿ğ—² ğ—£ğ—¼ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹: The RL exploration offers opportunities for further innovation in future iterations.
